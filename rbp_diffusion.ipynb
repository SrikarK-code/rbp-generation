{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yzkS7FVCPxa"
      },
      "source": [
        "rbp conditional latent diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNV6t61JGfZv"
      },
      "source": [
        "### mount gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlFIq2zSCcrx",
        "outputId": "6ac52ec5-bb7b-445e-ea97-c579ec85f4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq17nt0ACxOY"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPJO_Mx3QRJ9"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/rna-binding')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17A_zUAVCvK1",
        "outputId": "105ddf38-8815-48a7-f35a-31b537771e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-xGj78GIBo"
      },
      "source": [
        "### data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ176Aj0etVV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7Hl_M7zeVnA"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('final_attract_db_with_emb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6AMmpLfAr5x"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Load the dictionary back from the pickle file.\n",
        "with open(\"rbp_seqs_dict.pkl\", \"rb\") as f:\n",
        "    rbp_seqs_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBIK9NqBmux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rna_motif_emb = np.load('rna_motif_emb.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR5oupoyDg0_",
        "outputId": "4f59fac7-754a-4dd6-ba88-a4efb5d8be09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(rna_motif_emb[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlC-0ezvDnxN"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['rna_motif_emb', 'rbp_esm_emb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "476-y3RlEQGe"
      },
      "outputs": [],
      "source": [
        "data['rna_motif_emb'] = rna_motif_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek95MlFMEoTp"
      },
      "outputs": [],
      "source": [
        "data['rbp_esm_emb'] = data['RBP_sequence'].map(rbp_seqs_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r45nc1dZEuPy"
      },
      "outputs": [],
      "source": [
        "# Convert list of tensors to numpy array\n",
        "def tensors_to_numpy(tensor_list):\n",
        "    return np.stack([t.numpy() for t in tensor_list])\n",
        "\n",
        "# Apply the conversion to the 'rbp_esm_emb' column\n",
        "data['rbp_esm_emb'] = data['rbp_esm_emb'].apply(tensors_to_numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te0Kq4SNMybz",
        "outputId": "def6feb9-8018-439a-f6e1-672e3f6c06d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of RBP_sequence: 415.5270602706027\n"
          ]
        }
      ],
      "source": [
        "average_length = data['RBP_sequence'].apply(len).mean()\n",
        "print(f\"Average length of RBP_sequence: {average_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26Md0NQyNSjs"
      },
      "outputs": [],
      "source": [
        "data = data[data['RBP_sequence'].apply(len) <= 750]\n",
        "filtered_data = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEPzFUtoQguE",
        "outputId": "630c103e-b3a5-4523-d4e9-87ce98107530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum RBP sequence length: 741\n",
            "Maximum Motif length: 12\n"
          ]
        }
      ],
      "source": [
        "max_rbp_sequence_length = data['RBP_sequence'].apply(len).max()\n",
        "max_motif_length = data['Motif'].apply(len).max()\n",
        "\n",
        "print(f\"Maximum RBP sequence length: {max_rbp_sequence_length}\")\n",
        "print(f\"Maximum Motif length: {max_motif_length}\")\n",
        "\n",
        "# Use these lengths for padding\n",
        "max_encoder_seq_length = max_motif_length\n",
        "max_decoder_seq_length = max_rbp_sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from scipy import stats\n",
        "\n",
        "# # Assuming meta4k is a DataFrame and 'sequence' is the column with sequence strings\n",
        "# # Calculate sequence lengths\n",
        "# data['sequence_length'] = data['RBP_sequence'].apply(len)\n",
        "\n",
        "# # Compute Z-scores of the lengths\n",
        "# data['z_scores'] = stats.zscore(data['sequence_length'])\n",
        "\n",
        "# # Filter the outliers; this will keep only the rows that are not outliers\n",
        "# filtered_data = data[(data['z_scores'] > -3) & (data['z_scores'] < 3)]\n",
        "\n",
        "# # Determine the outliers\n",
        "# outliers = data[(data['z_scores'] <= -3) | (data['z_scores'] >= 3)]\n",
        "\n",
        "# # Print the outliers\n",
        "# print(outliers)\n"
      ],
      "metadata": {
        "id": "GAtrr0XEEH9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_rbp_sequence_length = filtered_data['RBP_sequence'].apply(len).max()\n",
        "min_rbp_sequence_length = filtered_data['RBP_sequence'].apply(len).min()\n",
        "max_motif_length = filtered_data['Motif'].apply(len).max()\n",
        "min_motif_length = filtered_data['Motif'].apply(len).min()\n",
        "\n",
        "print(f\"Maximum RBP sequence length: {max_rbp_sequence_length}\")\n",
        "print(f\"Maximum Motif length: {max_motif_length}\")\n",
        "print(f\"Minimum RBP sequence length: {min_rbp_sequence_length}\")\n",
        "print(f\"Minimum Motif length: {min_motif_length}\")\n",
        "\n",
        "# Use these lengths for padding\n",
        "max_encoder_seq_length = max_motif_length\n",
        "max_decoder_seq_length = max_rbp_sequence_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amcb3YWRGCHp",
        "outputId": "ec64363b-5335-4438-f6db-b15de19e0c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum RBP sequence length: 741\n",
            "Maximum Motif length: 12\n",
            "Minimum RBP sequence length: 136\n",
            "Minimum Motif length: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNLpP1fEsP0J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmBGN0NGMXm"
      },
      "source": [
        "### dataloader -- treat protein emebdding as the rna motif & treat molecular embedding as protein embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCGFpkbWGkCG",
        "outputId": "78dc97c5-78f0-48e7-f2ab-d2a2ace7cb96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Gene_name', 'Gene_id', 'Motif', 'RBP_sequence', 'rna_motif_emb',\n",
              "       'rbp_esm_emb'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzeaWlnVGpf6",
        "outputId": "94e427aa-5ca8-4032-9cf4-696336b4b72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 120)\n",
            "(594, 1280)\n"
          ]
        }
      ],
      "source": [
        "print(data['rna_motif_emb'][1].shape)\n",
        "print(data['rbp_esm_emb'][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = filtered_data.copy()"
      ],
      "metadata": {
        "id": "gRu3bA8ik8al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zev8fdq3H9ju",
        "outputId": "7d26368d-06bc-4b9f-fe63-64b5f17b7f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Checking the GPU availability\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError('GPU device not found')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnq_UyHM87nZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAJx8JeD87na",
        "outputId": "f22ce01a-febb-41f7-b3fb-8f04658daac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m92.2/93.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-32): 33 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install fair-esm\n",
        "##setting up ESM\n",
        "import torch\n",
        "import esm\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model.eval()  # disables dropout for deterministic results\n",
        "esm_model.cuda() #push model to gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "protein_input_list = [torch.tensor(seq, dtype=torch.float32) for seq in data['rna_motif_emb'].tolist()]\n",
        "molecule_input_list = [torch.tensor(seq, dtype=torch.float32) for seq in data['rbp_esm_emb'].tolist()]\n",
        "molecule_target_list = [torch.tensor(np.roll(seq, shift=-1, axis=0), dtype=torch.float32) for seq in data['rbp_esm_emb'].tolist()]"
      ],
      "metadata": {
        "id": "ohAQQLQV2mKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPTAJozd8B9J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, encoder_data, decoder_data, decoder_target, max_protein_seq_length, max_molecule_seq_length):\n",
        "        self.encoder_data = encoder_data\n",
        "        self.decoder_data = decoder_data\n",
        "        self.decoder_target = decoder_target\n",
        "        self.max_protein_seq_length = max_protein_seq_length\n",
        "        self.max_molecule_seq_length = max_molecule_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoder_seq = torch.nn.functional.pad(self.encoder_data[idx], (0, 0, 0, self.max_protein_seq_length - self.encoder_data[idx].size(0)))\n",
        "        decoder_seq = torch.nn.functional.pad(self.decoder_data[idx], (0, 0, 0, self.max_molecule_seq_length - self.decoder_data[idx].size(0)))\n",
        "        decoder_target = torch.nn.functional.pad(self.decoder_target[idx], (0, 0, 0, self.max_molecule_seq_length - self.decoder_target[idx].size(0)))\n",
        "        return encoder_seq, decoder_seq, decoder_target\n",
        "\n",
        "# Find the maximum sequence length for proteins and molecules\n",
        "max_protein_seq_length = max(seq.size(0) for seq in protein_input_list)\n",
        "max_molecule_seq_length = 750 #max(seq.size(0) for seq in molecule_input_list)\n",
        "\n",
        "# Create the dataset\n",
        "dataset = Seq2SeqDataset(protein_input_list, molecule_input_list, molecule_target_list, max_protein_seq_length, max_molecule_seq_length)\n",
        "\n",
        "# Splitting the dataset into training and test/validation sets\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size = len(dataset) - train_size  # 20% for testing\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoaders for the training and testing sets\n",
        "batch_size = 4  # You can adjust the batch size as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset[20][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWZf0VXj18mY",
        "outputId": "aedbaf20-48c4-49d9-9d25-1ababeceafba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjpJZyxn1_wa",
        "outputId": "7f642c08-4d81-4054-b872-5a772f58e402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([750, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### diffusion design module"
      ],
      "metadata": {
        "id": "TkwMqMYSmCt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transition module"
      ],
      "metadata": {
        "id": "MGirMYqZi8Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "-ucUFB0Ki5fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "variance scheduler"
      ],
      "metadata": {
        "id": "eBLbKhsIjALP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VarianceSchedule(nn.Module):\n",
        "\n",
        "    def __init__(self, num_steps=100, s=0.01):\n",
        "        super().__init__()\n",
        "        T = num_steps\n",
        "        t = torch.arange(0, num_steps+1, dtype=torch.float)\n",
        "        f_t = torch.cos( (np.pi / 2) * ((t/T) + s) / (1 + s) ) ** 2\n",
        "        alpha_bars = f_t / f_t[0]\n",
        "\n",
        "        betas = 1 - (alpha_bars[1:] / alpha_bars[:-1])\n",
        "        betas = torch.cat([torch.zeros([1]), betas], dim=0)\n",
        "        betas = betas.clamp_max(0.999)\n",
        "\n",
        "        sigmas = torch.zeros_like(betas)\n",
        "        for i in range(1, betas.size(0)):\n",
        "            sigmas[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
        "        sigmas = torch.sqrt(sigmas)\n",
        "\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alpha_bars', alpha_bars)\n",
        "        self.register_buffer('alphas', 1 - betas)\n",
        "        self.register_buffer('sigmas', sigmas)\n",
        "        # calculate X0 = sqrt_recip_alphas_cumprod * Xt - sqrt_recipm1_alphas_cumprod * noise\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alpha_bars))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alpha_bars - 1))\n",
        "\n",
        "        alphas = 1 - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
        "        self.posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n",
        "        self.posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "        self.posterior_mean_coef2 = (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)\n",
        "\n",
        "    def to(self,device):\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                self.__dict__[k] = v.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "iNakb_LUi-yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rbp continuous transition -- adapted from small molecule version"
      ],
      "metadata": {
        "id": "BeejLYVKjDbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinEmbeddingTransformation(nn.Module):\n",
        "    def __init__(self, protein_embedding_dim, output_embedding_dim, target_seq_length):\n",
        "        super().__init__()\n",
        "        self.target_seq_length = target_seq_length\n",
        "        self.rnn = nn.GRU(protein_embedding_dim, output_embedding_dim, batch_first=True)\n",
        "        self.fc_final = nn.Linear(output_embedding_dim, output_embedding_dim)\n",
        "\n",
        "    def forward(self, protein_embedding):\n",
        "        # GRU Layer expects input of shape (batch, seq_len, input_size)\n",
        "        # Assuming protein_embedding is already in this shape\n",
        "        output, _ = self.rnn(protein_embedding)\n",
        "\n",
        "        # Taking the output corresponding to the last timestep\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Repeating the last output to match the target sequence length\n",
        "        last_output_repeated = last_output.unsqueeze(1).repeat(1, self.target_seq_length, 1)\n",
        "\n",
        "        # Passing through a final Linear layer\n",
        "        transformed_embedding = self.fc_final(last_output_repeated)\n",
        "\n",
        "        return transformed_embedding\n",
        "\n",
        "\n",
        "class SmallMoleculeContinuousTransition(nn.Module):\n",
        "    def __init__(self, num_steps, embedding_dim, protein_embedding_dim, noise_mean, noise_std, var_sched_opt={}):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.protein_embedding_dim = protein_embedding_dim\n",
        "        self.var_sched = VarianceSchedule(num_steps, **var_sched_opt)\n",
        "        self.noise_mean = noise_mean\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "        # Layers for adding noise\n",
        "        self.add_noise_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # Denoising layers, now also conditioned on protein embeddings\n",
        "        self.denoise_layer = nn.Linear(embedding_dim + protein_embedding_dim, embedding_dim)\n",
        "        self.denoise_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + protein_embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def add_noise(self, molecule_embedding, t):\n",
        "        # Add noise based on the timestep 't'\n",
        "        alpha_bar = self.var_sched.alpha_bars[t].unsqueeze(-1).unsqueeze(-1)\n",
        "        noise = torch.randn_like(molecule_embedding) * self.noise_std + self.noise_mean\n",
        "        noised_embedding = alpha_bar * molecule_embedding + (1 - alpha_bar) * self.add_noise_layer(noise)\n",
        "        return noised_embedding\n",
        "\n",
        "    def denoise(self, noised_embedding, protein_embedding, t):\n",
        "        # Instantiate the transformation module\n",
        "        transform_module = ProteinEmbeddingTransformation(\n",
        "            protein_embedding_dim=self.protein_embedding_dim,\n",
        "            output_embedding_dim=120,  # Adjust as needed\n",
        "            target_seq_length=noised_embedding.size(1)  # This is 750 in your case\n",
        "        ).to(protein_embedding.device)\n",
        "\n",
        "        # Transform protein_embedding to match the sequence length\n",
        "        transformed_protein_embedding = transform_module(protein_embedding)\n",
        "\n",
        "        # Now concatenate with noised_embedding\n",
        "        combined_embedding = torch.cat((noised_embedding, transformed_protein_embedding), dim=2)\n",
        "        alpha_bar = self.var_sched.alpha_bars[t].unsqueeze(-1).unsqueeze(-1)\n",
        "        processed_embedding = self.denoise_layer(combined_embedding)\n",
        "        mlp_output = self.denoise_mlp(combined_embedding)\n",
        "        predicted_embedding = alpha_bar * (mlp_output + processed_embedding) + (1 - alpha_bar) * noised_embedding\n",
        "        return predicted_embedding\n"
      ],
      "metadata": {
        "id": "FEmNjVpQjCBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "full_dpm & epsilon_net"
      ],
      "metadata": {
        "id": "TEzlryenjiHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class EpsilonNet(nn.Module):\n",
        "#     def __init__(self, molecule_embedding_dim, protein_embedding_dim):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(protein_embedding_dim, 512)\n",
        "#         self.fc2 = nn.Linear(512, 512)\n",
        "#         self.fc3 = nn.Linear(512, molecule_embedding_dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#     def forward(self, protein_embedding):\n",
        "#         x = self.relu(self.fc1(protein_embedding))\n",
        "#         print('x after relu1 epsnet')\n",
        "#         print(x.shape)\n",
        "#         x = self.relu(self.fc2(x))\n",
        "#         print('x after relu2 epsnet')\n",
        "#         print(x.shape)\n",
        "#         epsilon = self.fc3(x)\n",
        "#         print('after fc3 epsnet -- EPSILON -- SHOUDL HAVE 1280 DIM')\n",
        "#         print(epsilon.shape)\n",
        "#         return epsilon"
      ],
      "metadata": {
        "id": "Bv5vvUUJjloq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class EpsilonNet(nn.Module):\n",
        "#     def __init__(self, molecule_embedding_dim, protein_embedding_dim, molecule_seqlength):\n",
        "#         super().__init__()\n",
        "#         self.molecule_seqlength = molecule_seqlength\n",
        "#         self.fc1 = nn.GRU(protein_embedding_dim, 512, batch_first=True)\n",
        "#         self.fc2 = nn.Linear(512, 512)\n",
        "#         self.rnn = nn.GRU(512, molecule_embedding_dim, batch_first=True)\n",
        "#         self.fc_final = nn.Linear(molecule_embedding_dim, molecule_embedding_dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#     def forward(self, protein_embedding):\n",
        "#         # GRU Layer expects input of shape (batch, seq_len, input_size)\n",
        "#         # Assuming protein_embedding is already in this shape\n",
        "#         output, _ = self.fc1(protein_embedding)\n",
        "\n",
        "#         # Process the output of GRU layer\n",
        "#         # Taking the output corresponding to the last timestep\n",
        "#         last_output = output[:, -1, :]\n",
        "\n",
        "#         # Passing through Linear layer and ReLU\n",
        "#         x = self.fc2(last_output)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         # Preparing input for the second GRU layer\n",
        "#         # Repeating x along the time dimension to match molecule_seqlength\n",
        "#         x_repeated = x.unsqueeze(1).repeat(1, self.molecule_seqlength, 1)\n",
        "\n",
        "#         # Second GRU layer\n",
        "#         output, _ = self.rnn(x_repeated)\n",
        "\n",
        "#         # Final Linear layer\n",
        "#         epsilon = self.fc_final(output)\n",
        "\n",
        "#         return epsilon\n"
      ],
      "metadata": {
        "id": "sDWktiw8ocGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EpsilonNet(nn.Module):\n",
        "    def __init__(self, molecule_embedding_dim, protein_embedding_dim, molecule_seqlength):\n",
        "        super().__init__()\n",
        "        self.molecule_seqlength = molecule_seqlength\n",
        "        self.fc1 = nn.Linear(protein_embedding_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, molecule_embedding_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, molecule_seqlength, molecule_embedding_dim))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, protein_embedding):\n",
        "        # Flatten protein_embedding if it's 3D (batch, seq_len, input_size) to 2D (batch, input_size)\n",
        "        if protein_embedding.dim() == 3:\n",
        "            protein_embedding = protein_embedding.view(protein_embedding.size(0), -1)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = self.fc1(protein_embedding)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Expand the output to match the molecule sequence length\n",
        "        x = self.fc3(x)\n",
        "        x = x.unsqueeze(1).expand(-1, self.molecule_seqlength, -1)\n",
        "\n",
        "        # Add positional encodings to give each position in the sequence a unique signature\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qMbA60OD0ywF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonNet(nn.Module):\n",
        "    def __init__(self, molecule_embedding_dim, protein_embedding_dim, molecule_seqlength):\n",
        "        super().__init__()\n",
        "        self.molecule_seqlength = molecule_seqlength\n",
        "        self.gru = nn.GRU(protein_embedding_dim, 512, batch_first=True)\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, molecule_embedding_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, molecule_seqlength, molecule_embedding_dim))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, protein_embedding):\n",
        "        # Process protein_embedding through GRU\n",
        "        output, _ = self.gru(protein_embedding)\n",
        "\n",
        "        # Use the output of the last timestep\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = self.relu(self.fc1(last_output))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Expand the output to match the molecule sequence length\n",
        "        x = x.unsqueeze(1).expand(-1, self.molecule_seqlength, -1)\n",
        "\n",
        "        # Add positional encodings to give each position in the sequence a unique signature\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8gt78sto48in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FullDPM(nn.Module):\n",
        "    def __init__(self, molecule_embedding_dim, protein_embedding_dim, num_steps, noise_mean, noise_std, molecule_seqlength):\n",
        "        super().__init__()\n",
        "        self.molecule_embedding_dim = molecule_embedding_dim\n",
        "        self.protein_embedding_dim = protein_embedding_dim\n",
        "        self.num_steps = num_steps\n",
        "        self.molecule_seqlength = molecule_seqlength  # Add molecule_seqlength as an attribute\n",
        "        self.epsilon_net = EpsilonNet(molecule_embedding_dim, protein_embedding_dim, molecule_seqlength)\n",
        "        self.transition_model = SmallMoleculeContinuousTransition(num_steps, molecule_embedding_dim, protein_embedding_dim, noise_mean, noise_std)\n",
        "\n",
        "\n",
        "    def forward(self, protein_embedding, t):\n",
        "        # Start with noise\n",
        "        molecule_seqlength = 750\n",
        "        molecule_embedding = torch.randn(protein_embedding.size(0), molecule_seqlength, self.molecule_embedding_dim, device=protein_embedding.device)\n",
        "        print('fulldpmforward-moleuclceembdding')\n",
        "        print(molecule_embedding.shape)\n",
        "        noised_embedding = self.transition_model.add_noise(molecule_embedding, t)\n",
        "        print(\"fulldpm forward noised_embedding_done...\")\n",
        "        print('shape of protein_embedding before epsilon below...')\n",
        "        print(protein_embedding.shape)\n",
        "        epsilon = self.epsilon_net(protein_embedding)\n",
        "        print('fulldpm-forward-noiseembedding')\n",
        "        print(noised_embedding.shape)\n",
        "        print('fulldpm-forward-epsilon')\n",
        "        print(epsilon.shape)\n",
        "        print('fulldpm-forward-proteinemebdding')\n",
        "        print(protein_embedding.shape)\n",
        "        print('fulldpm-forward-t')\n",
        "        print(t)\n",
        "        denoised_embedding = self.transition_model.denoise(noised_embedding + epsilon, protein_embedding, t)\n",
        "        return denoised_embedding\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, protein_embedding):\n",
        "        molecule_embedding = torch.randn(protein_embedding.size(0), self.molecule_embedding_dim, device=protein_embedding.device)\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            molecule_embedding = self.forward(protein_embedding, t)\n",
        "        return molecule_embedding\n"
      ],
      "metadata": {
        "id": "vqdO8fUhjngW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "design module"
      ],
      "metadata": {
        "id": "VAgeEkG1jp2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionProteinToMolecule(nn.Module):\n",
        "    def __init__(self, cfg, num_protein_features, num_molecule_features):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.diffusion = FullDPM(\n",
        "            num_molecule_features,\n",
        "            num_protein_features,\n",
        "            cfg['num_steps'],\n",
        "            cfg['noise_mean'],\n",
        "            cfg['noise_std'],\n",
        "            cfg['molecule_seqlength']  # Add this line\n",
        "        )\n",
        "\n",
        "    def forward(self, protein_emb, t=None):\n",
        "        if t is None:\n",
        "            t = torch.randint(0, self.cfg['num_steps'], (1,), dtype=torch.long)\n",
        "        return self.diffusion(protein_emb, t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, protein_emb):\n",
        "        return self.diffusion.sample(protein_emb)\n"
      ],
      "metadata": {
        "id": "yAwte04Yjqz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "VzO-8q5fjtbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Configuration for the diffusion model\n",
        "cfg = {\n",
        "    'num_steps': 100,  # Number of steps in the diffusion process\n",
        "    'noise_mean': 0.0,\n",
        "    'noise_std': 1.0,\n",
        "    'molecule_seqlength': 750  # Adjust to the desired molecule sequence length\n",
        "}\n",
        "\n",
        "\n",
        "num_protein_features = 120  # dimension of motif embeddings\n",
        "num_molecule_features = 1280  # output dimension of rbp embeddings\n",
        "\n",
        "# Initialize the model\n",
        "model = DiffusionProteinToMolecule(cfg, num_protein_features, num_molecule_features)\n",
        "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-g4fNy9juKt",
        "outputId": "4131b969-4f2d-41da-a59e-8d465219b39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DiffusionProteinToMolecule(\n",
              "  (diffusion): FullDPM(\n",
              "    (epsilon_net): EpsilonNet(\n",
              "      (fc1): Linear(in_features=120, out_features=512, bias=True)\n",
              "      (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (fc3): Linear(in_features=512, out_features=1280, bias=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (transition_model): SmallMoleculeContinuousTransition(\n",
              "      (var_sched): VarianceSchedule()\n",
              "      (add_noise_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      (denoise_layer): Linear(in_features=1400, out_features=1280, bias=True)\n",
              "      (denoise_mlp): Sequential(\n",
              "        (0): Linear(in_features=1400, out_features=1280, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avXTjb7vj3kQ",
        "outputId": "92fd86de-4230-4a67-db53-e39b2a3919fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBwEpnOtxGTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset[0][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0MDcdh8xB5w",
        "outputId": "7501922c-0616-42ed-acdc-17a48490c249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([750, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Xit2z2axHA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (protein_emb, true_molecule_emb, *_) in enumerate(train_loader):\n",
        "        protein_emb = protein_emb.to(device)\n",
        "        print('proteinembshape')\n",
        "        print(protein_emb.shape)\n",
        "        print('moleculeembshape')\n",
        "        print(true_molecule_emb.shape)\n",
        "        true_molecule_emb = true_molecule_emb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        generated_molecule_emb = model(protein_emb)\n",
        "        print('generatedmoleucleshape')\n",
        "        print(generated_molecule_emb.shape)\n",
        "        loss = criterion(generated_molecule_emb, true_molecule_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for protein_emb, true_molecule_emb, _ in val_loader:  # Unpack all three elements\n",
        "            protein_emb = protein_emb.to(device)\n",
        "            true_molecule_emb = true_molecule_emb.to(device)\n",
        "            generated_molecule_emb = model(protein_emb)\n",
        "            loss = criterion(generated_molecule_emb, true_molecule_emb)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # use gradient clipping -- combats exploding gradients\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "# criterion = torch.nn.MSELoss()\n",
        "criterion = torch.nn.L1Loss()\n",
        "\n",
        "\n",
        "# Training execution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = validate(model, test_loader, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "ySiQoN7Hj4iW",
        "outputId": "83f5697d-8643-46f3-e6f1-a299bb987026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proteinembshape\n",
            "torch.Size([4, 12, 120])\n",
            "moleculeembshape\n",
            "torch.Size([4, 750, 1280])\n",
            "fulldpmforward-moleuclceembdding\n",
            "torch.Size([4, 750, 1280])\n",
            "fulldpm forward noised_embedding_done...\n",
            "shape of protein_embedding before epsilon below...\n",
            "torch.Size([4, 12, 120])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-f0ae30a1280e>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-f0ae30a1280e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrue_molecule_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_molecule_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgenerated_molecule_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generatedmoleucleshape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_molecule_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-693ddfad0e42>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_emb, t)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-bb29e0775dce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_embedding, t)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape of protein_embedding before epsilon below...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fulldpm-forward-noiseembedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoised_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-7b356864fa96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_embedding)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Pass through fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x1440 and 120x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYsBaV7Lj9sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating output embeddings"
      ],
      "metadata": {
        "id": "1X0kSnGrj_oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "generated_embeddings = []\n",
        "ground_truth_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for protein_emb, molecule_emb,_ in test_loader:\n",
        "        protein_emb = protein_emb.to(device)\n",
        "        molecule_emb = molecule_emb.to(device)\n",
        "        output = model(protein_emb)\n",
        "        generated_embeddings.extend(output.cpu().numpy())\n",
        "        ground_truth_embeddings.extend(molecule_emb.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "generated_embeddings = np.array(generated_embeddings)\n",
        "ground_truth_embeddings = np.array(ground_truth_embeddings)\n",
        "len(ground_truth_embeddings)"
      ],
      "metadata": {
        "id": "uSImZ24EkDSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run PCA\n",
        "pca = PCA(n_components=2) # Change to n_components=3 for 3D\n",
        "combined_embeddings = np.concatenate((generated_embeddings, ground_truth_embeddings))\n",
        "pca_result = pca.fit_transform(combined_embeddings)\n",
        "\n",
        "# Split the PCA result back into generated and ground truth\n",
        "split_point = generated_embeddings.shape[0]\n",
        "generated_pca = pca_result[:split_point, :]\n",
        "ground_truth_pca = pca_result[split_point:, :]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(generated_pca[:, 0], generated_pca[:, 1], label='Generated Embeddings', alpha=0.5)\n",
        "plt.scatter(ground_truth_pca[:, 0], ground_truth_pca[:, 1], label='Ground Truth Embeddings', alpha=0.5)\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('PCA of Generated vs Ground Truth Embeddings')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-mWlbOGCkE9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Running t-SNE on the combined embeddings\n",
        "tsne = TSNE(n_components=2, random_state=42)  # set random_state for reproducibility\n",
        "tsne_result = tsne.fit_transform(combined_embeddings)\n",
        "\n",
        "# Split the t-SNE result back into generated and ground truth embeddings\n",
        "generated_tsne = tsne_result[:split_point, :]\n",
        "ground_truth_tsne = tsne_result[split_point:, :]\n",
        "\n",
        "# Plotting the t-SNE results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(generated_tsne[:, 0], generated_tsne[:, 1], label='Generated Embeddings', alpha=0.5)\n",
        "plt.scatter(ground_truth_tsne[:, 0], ground_truth_tsne[:, 1], label='Ground Truth Embeddings', alpha=0.5)\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE of Generated vs Ground Truth Embeddings')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "srjjNfS5kGUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarities = cosine_similarity(generated_embeddings, ground_truth_embeddings)\n",
        "cosine_similarities_diag = np.diag(cosine_similarities)\n",
        "average_cosine_similarity = np.mean(cosine_similarities_diag)"
      ],
      "metadata": {
        "id": "NgSNPrWPkHsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_cosine_similarity"
      ],
      "metadata": {
        "id": "onhpEfMMkIo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "TNV6t61JGfZv",
        "Fy-xGj78GIBo",
        "ijmBGN0NGMXm"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}